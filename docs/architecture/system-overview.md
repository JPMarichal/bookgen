# System Architecture Overview

Complete technical overview of BookGen's architecture and design.

## ğŸ—ï¸ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        BookGen System                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚   Client Apps  â”‚â”€â”€â”€â–¶â”‚  FastAPI REST  â”‚â”€â”€â”€â–¶â”‚  Celery Worker â”‚â”‚
â”‚  â”‚  (Web, CLI)    â”‚â—€â”€â”€â”€â”‚      API       â”‚â—€â”€â”€â”€â”‚   (Async Jobs) â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚         â”‚                      â”‚                      â”‚          â”‚
â”‚         â”‚                      â”‚                      â”‚          â”‚
â”‚    WebSocket              â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â–¼â”€â”€â”€â”      â”‚
â”‚    Real-time              â”‚PostgreSQLâ”‚            â”‚ Redis â”‚      â”‚
â”‚    Updates                â”‚Database  â”‚            â”‚ Queue â”‚      â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                        â”‚          â”‚
â”‚                                                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”‚
â”‚                                                   â”‚OpenRouterâ”‚    â”‚
â”‚                                                   â”‚   API    â”‚    â”‚
â”‚                                                   â”‚  (LLM)   â”‚    â”‚
â”‚                                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Core Components

### 1. FastAPI REST API

**Purpose:** Primary entry point for all requests

**Responsibilities:**
- Accept and validate API requests
- Handle authentication/authorization
- Manage WebSocket connections
- Route requests to appropriate services
- Return responses to clients

**Technology:**
- FastAPI 0.104+
- Uvicorn ASGI server
- Pydantic for validation
- Python 3.9+

**Key Files:**
```
src/main.py                     # Application entry point
src/api/routers/
  â”œâ”€â”€ biographies.py           # Biography endpoints
  â”œâ”€â”€ sources.py               # Source validation
  â”œâ”€â”€ metrics.py               # Metrics endpoint
  â””â”€â”€ websocket.py             # WebSocket handler
src/api/middleware/
  â”œâ”€â”€ rate_limiter.py          # Rate limiting
  â””â”€â”€ request_logger.py        # Request logging
```

### 2. Celery Worker

**Purpose:** Asynchronous task processing

**Responsibilities:**
- Execute long-running biography generation
- Process tasks in background
- Handle retries and failures
- Update job status and progress
- Send notifications

**Technology:**
- Celery 5.3+
- Redis as message broker
- PostgreSQL for result backend

**Key Files:**
```
src/worker.py                  # Celery app configuration
src/tasks/
  â”œâ”€â”€ biography_tasks.py       # Main generation tasks
  â”œâ”€â”€ validation_tasks.py      # Source validation
  â””â”€â”€ export_tasks.py          # Document export
```

### 3. State Machine Engine

**Purpose:** Orchestrate biography generation workflow

**Responsibilities:**
- Manage generation phases
- Track workflow state
- Handle checkpoints
- Coordinate services
- Ensure consistency

**Phases:**
1. Source Validation
2. Research Planning
3. Chapter Generation
4. Special Sections
5. Quality Control
6. Export

**Key Files:**
```
src/engine/
  â”œâ”€â”€ bookgen_engine.py        # Main engine
  â”œâ”€â”€ state_machine.py         # State management
  â””â”€â”€ workflow_manager.py      # Workflow coordination
```

### 4. OpenRouter Client

**Purpose:** Interface with AI language models

**Responsibilities:**
- Send generation requests to OpenRouter API
- Handle API responses
- Manage rate limiting
- Implement retry logic
- Track token usage

**Supported Models:**
- Qwen 2.5 (Free tier)
- Claude 3.5 Sonnet (Premium)
- GPT-4 Turbo (Premium)
- Gemini Pro (Premium)

**Key Files:**
```
src/services/openrouter_client.py
src/services/prompt_templates.py
```

### 5. PostgreSQL Database

**Purpose:** Persistent data storage

**Responsibilities:**
- Store job metadata
- Track biography progress
- Store generated content
- Manage relationships
- Provide ACID guarantees

**Schema:**
```sql
-- Jobs table
CREATE TABLE biography_jobs (
    job_id UUID PRIMARY KEY,
    character VARCHAR(255) NOT NULL,
    status VARCHAR(50) NOT NULL,
    progress FLOAT DEFAULT 0.0,
    current_phase VARCHAR(100),
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    completed_at TIMESTAMP,
    error_message TEXT
);

-- Chapters table
CREATE TABLE chapters (
    id SERIAL PRIMARY KEY,
    job_id UUID REFERENCES biography_jobs(job_id),
    chapter_number INTEGER,
    title VARCHAR(255),
    content TEXT,
    word_count INTEGER,
    created_at TIMESTAMP
);

-- Sources table
CREATE TABLE sources (
    id SERIAL PRIMARY KEY,
    job_id UUID REFERENCES biography_jobs(job_id),
    url TEXT NOT NULL,
    valid BOOLEAN,
    status_code INTEGER,
    relevance_score FLOAT,
    validated_at TIMESTAMP
);
```

**Key Files:**
```
src/database/
  â”œâ”€â”€ connection.py            # DB connection
  â”œâ”€â”€ models.py                # SQLAlchemy models
  â””â”€â”€ repositories/            # Data access layer
alembic/                       # Migrations
  â””â”€â”€ versions/
```

### 6. Redis

**Purpose:** Message broker and cache

**Responsibilities:**
- Queue Celery tasks
- Store task results
- Cache frequently accessed data
- Manage rate limiting
- Handle session data

**Redis Databases:**
- DB 0: Task queue (Celery)
- DB 1: Task results
- DB 2: Cache
- DB 3: Rate limiting

**Key Configuration:**
```bash
REDIS_HOST=localhost
REDIS_PORT=6379
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/1
```

### 7. Notification Service

**Purpose:** Multi-channel notifications

**Channels:**
- **WebSocket**: Real-time browser updates
- **Webhook**: HTTP callbacks to external services
- **Email**: SMTP notifications

**Key Files:**
```
src/services/notifications.py
src/api/routers/websocket.py
src/email/
  â”œâ”€â”€ templates/               # Email templates
  â””â”€â”€ smtp_client.py           # SMTP sender
```

## ğŸ”„ Data Flow

### Biography Generation Flow

```
1. Client Request
   â†“
2. FastAPI validates request
   â†“
3. Create job in database (status: pending)
   â†“
4. Queue Celery task
   â†“
5. Celery worker picks up task
   â†“
6. State Machine Engine starts
   â†“
7. Phase 1: Validate sources
   â”œâ”€ Fetch each URL
   â”œâ”€ Analyze content
   â””â”€ Store results
   â†“
8. Phase 2: Research planning
   â”œâ”€ Extract key facts
   â”œâ”€ Identify events
   â””â”€ Create outline
   â†“
9. Phase 3: Generate chapters
   â”œâ”€ For each chapter:
   â”‚  â”œâ”€ Build prompt
   â”‚  â”œâ”€ Call OpenRouter API
   â”‚  â”œâ”€ Validate response
   â”‚  â””â”€ Store chapter
   â””â”€ Update progress
   â†“
10. Phase 4: Generate special sections
    â”œâ”€ Prologue/Epilogue
    â”œâ”€ Timeline
    â””â”€ Glossary
    â†“
11. Phase 5: Quality control
    â”œâ”€ Validate word counts
    â”œâ”€ Check coherence
    â””â”€ Verify citations
    â†“
12. Phase 6: Export
    â”œâ”€ Concatenate sections
    â”œâ”€ Apply template
    â””â”€ Generate .docx
    â†“
13. Update job (status: completed)
    â†“
14. Send notifications
    â”œâ”€ WebSocket update
    â”œâ”€ Webhook callback
    â””â”€ Email (if configured)
```

### Request/Response Flow

```
Client App
    â”‚
    â”‚ HTTP POST /api/v1/biographies
    â†“
FastAPI Router
    â”‚
    â”‚ Validate request with Pydantic
    â†“
Biography Service
    â”‚
    â”‚ Create job record
    â†“
PostgreSQL
    â”‚
    â”‚ Job saved (status: pending)
    â†“
Celery Task Queue
    â”‚
    â”‚ Task queued in Redis
    â†“
Celery Worker
    â”‚
    â”‚ Pick up task
    â†“
BookGen Engine
    â”‚
    â”‚ Execute workflow
    â”‚ â”œâ”€ Update progress
    â”‚ â”œâ”€ Send notifications
    â”‚ â””â”€ Save checkpoints
    â†“
OpenRouter API
    â”‚
    â”‚ Generate content
    â†“
Storage Layer
    â”‚
    â”‚ Save chapters/files
    â†“
Client App
    â”‚
    â”‚ Receives notifications via:
    â”œâ”€ WebSocket (real-time)
    â”œâ”€ Webhook (callback)
    â””â”€ Email (optional)
```

## ğŸ” Security Architecture

### Authentication & Authorization

**Current:**
- Optional API key authentication
- IP-based rate limiting
- CORS protection

**Production Recommendations:**
- JWT token authentication
- Role-based access control (RBAC)
- OAuth 2.0 integration

### Data Security

**In Transit:**
- TLS/SSL encryption (HTTPS)
- WebSocket over TLS (WSS)
- Encrypted webhook payloads

**At Rest:**
- Database encryption
- Encrypted environment variables
- Secure credential storage

### API Security

**Rate Limiting:**
```python
# Default: 60 requests per minute per IP
RATE_LIMIT_PER_MINUTE=60
```

**Input Validation:**
- Pydantic models
- URL sanitization
- SQL injection prevention
- XSS protection

**Security Headers:**
```python
# Added by middleware
X-Content-Type-Options: nosniff
X-Frame-Options: DENY
X-XSS-Protection: 1; mode=block
```

## ğŸ“ˆ Scalability

### Horizontal Scaling

**API Layer:**
```bash
# Run multiple API instances behind load balancer
docker-compose scale api=3
```

**Worker Layer:**
```bash
# Run multiple workers for parallel processing
docker-compose scale worker=5
```

**Load Balancer:**
```
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  Nginx  â”‚
       â”‚(LB/Proxy)â”‚
       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
            â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”
    â”‚       â”‚       â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”
â”‚API-1 â”‚ â”‚API-2â”‚ â”‚API-3â”‚
â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜
```

### Vertical Scaling

**Database:**
- Increase PostgreSQL resources
- Tune connection pooling
- Add read replicas

**Redis:**
- Increase memory allocation
- Enable persistence
- Configure eviction policies

**Workers:**
- Increase concurrency per worker
- Allocate more CPU/RAM
- Optimize batch sizes

### Caching Strategy

**Redis Cache:**
```python
# Cache validation results
cache_key = f"source:valid:{url_hash}"
redis.setex(cache_key, 3600, validation_result)

# Cache API responses
cache_key = f"status:{job_id}"
redis.setex(cache_key, 60, job_status)
```

**Application Cache:**
- Source validation results (1 hour)
- Job status (1 minute)
- System metrics (5 minutes)

## ğŸ” Monitoring & Observability

### Health Checks

**Endpoints:**
- `/health` - Basic health
- `/api/v1/status` - Detailed status
- `/api/v1/metrics` - System metrics

**Metrics Tracked:**
- Active jobs
- Completed jobs
- Failed jobs
- Average generation time
- API response times
- Worker queue length

### Logging

**Log Levels:**
- DEBUG: Development details
- INFO: Normal operations
- WARNING: Potential issues
- ERROR: Errors that need attention
- CRITICAL: System failures

**Log Aggregation:**
```bash
# All logs to stdout (Docker)
docker-compose logs -f

# Structured JSON logs
LOG_FORMAT=json

# Log levels by service
API_LOG_LEVEL=INFO
WORKER_LOG_LEVEL=DEBUG
```

### Alerting

**Health Monitoring:**
```bash
# Automated health checks every 5 minutes
*/5 * * * * curl -f http://localhost:8000/health || alert
```

**Error Alerting:**
- Email on critical errors
- Slack/Discord webhooks
- PagerDuty integration

## ğŸš€ Deployment Architecture

### Docker Deployment

```yaml
services:
  nginx:        # Reverse proxy
  api:          # FastAPI application
  worker:       # Celery worker
  db:           # PostgreSQL
  redis:        # Redis server
```

### Production Stack

```
Internet
    â”‚
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cloudflare â”‚  (CDN, DDoS protection)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚    Nginx    â”‚  (Reverse proxy, SSL, load balancing)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Docker    â”‚  (Container orchestration)
â”‚  Containers â”‚
â”‚  â”œâ”€ API (3x)â”‚
â”‚  â”œâ”€ Worker  â”‚
â”‚  â”œâ”€ DB      â”‚
â”‚  â””â”€ Redis   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Performance Characteristics

### Throughput

**API:**
- 100+ requests/second (single instance)
- 300+ requests/second (3 instances)

**Worker:**
- 1-3 concurrent biography jobs
- 5-10 concurrent chapters per job

### Latency

**API Endpoints:**
- Health check: < 10ms
- Job creation: < 100ms
- Status query: < 50ms

**Generation:**
- Source validation: 5-10 minutes
- Chapter generation: 1-2 hours
- Total: 2-4 hours

### Resource Usage

**Per Biography Job:**
- CPU: 2-4 cores during generation
- RAM: 1-2 GB
- Disk: ~10 MB output
- Network: ~50 MB API calls

## ğŸ”„ Technology Stack

**Backend:**
- Python 3.9+
- FastAPI 0.104+
- Celery 5.3+
- SQLAlchemy 2.0+
- Pydantic 2.5+

**Databases:**
- PostgreSQL 13+
- Redis 6+

**Infrastructure:**
- Docker 20.10+
- Docker Compose 1.29+
- Nginx 1.21+

**External Services:**
- OpenRouter API
- SMTP (Gmail, SendGrid)
- Let's Encrypt (SSL)

**Development:**
- pytest (testing)
- Alembic (migrations)
- Black (formatting)
- Ruff (linting)

---

[â† User Guide](../user-guide/notifications.md) | [Component Details â†’](components.md)
